{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/HnuCIx1TbGPyqSHItzFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GGPengZX/AI_Test/blob/main/Mixtral_8x7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhZdyMOySL7B",
        "outputId": "a0bf6f94-4449-485d-fbbb-159750438db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;33mUsing Python: /usr/bin/python3 \u001b[0m\n",
            "WARNING - Experimental Option Selected: plugins\n",
            "WARNING - plugins option may change later\n",
            "INFO    - Compatible with current configuration\n",
            "INFO    - Running Uninstaller\n",
            "WARNING - Uninstaller did not find previous installation\n",
            "WARNING - SHELL variable not found. Using bash as SHELL\n",
            "INFO    - shell configuration updated\n",
            "INFO    - Downloading WasmEdge\n",
            "|============================================================|100.00 %INFO    - Downloaded\n",
            "INFO    - Installing WasmEdge\n",
            "INFO    - WasmEdge Successfully installed\n",
            "INFO    - Downloading Plugin: wasi_nn-ggml-cuda\n",
            "|============================================================|100.00 %INFO    - Downloaded\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "INFO    - Run:\n",
            "source /root/.bashrc\n"
          ]
        }
      ],
      "source": [
        "!curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugin wasi_nn-ggml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LO https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjFDDvkoSYxa",
        "outputId": "4e8591bf-bc84-4552-d72e-37cbdd3615cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1180  100  1180    0     0   4255      0 --:--:-- --:--:-- --:--:--  4244\n",
            "100 30.0G  100 30.0G    0     0   184M      0  0:02:46  0:02:46 --:--:-- 66.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LO https://github.com/second-state/llama-utils/raw/main/chat/llama-chat.wasm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RhvzCZBSg-6",
        "outputId": "87f42e05-30c4-4814-c232-e8a53ca3624f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  223k    0  223k    0     0   597k      0 --:--:-- --:--:-- --:--:--  597k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LO https://github.com/second-state/llama-utils/raw/main/api-server/llama-api-server.wasm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLj8Dm_SSlux",
        "outputId": "27503c24-6460-47a9-9324-42ca2fe717a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  223k    0  223k    0     0   631k      0 --:--:-- --:--:-- --:--:--  630k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LO https://github.com/second-state/chatbot-ui/releases/download/v0.1.0/chatbot-ui.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbQVaIasTYxy",
        "outputId": "b644cd4c-9fd4-41a7-b919-c25d443300ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1719k  100 1719k    0     0  2068k      0 --:--:-- --:--:-- --:--:-- 2068k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xzf chatbot-ui.tar.gz"
      ],
      "metadata": {
        "id": "PUBRaonCTdsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm chatbot-ui.tar.gz"
      ],
      "metadata": {
        "id": "xt4rFHcnTgIw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wasmedge --dir .:. --nn-preload default:GGML:AUTO:mixtral-8x7b-instruct-v0.1.Q5_0.gguf llama-api-server.wasm -p mistral-instruct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jID7VvL2ToWY",
        "outputId": "75d43475-f653-4baa-d5af-fb3eec3fbca4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: wasmedge: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5FfdQSabTsuY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}